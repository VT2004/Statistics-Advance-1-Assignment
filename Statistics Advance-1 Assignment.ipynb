{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLS79fL/8evX6p7aPI44N7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1. Explain the properties of the F-distribution."],"metadata":{"id":"as1bEKsvOp23"}},{"cell_type":"markdown","source":["1. Shape: The F-distribution is right-skewed and only takes positive values. The shape depends on two parameters: the numerator and denominator degrees of freedom.\n","\n","2. Dependence on Degrees of Freedom: The F-distribution has two types of degrees of freedom:\n","\n","Numerator (dâ‚): Linked to the first sample variance.\n","Denominator (dâ‚‚): Linked to the second sample variance.\n","3. Mean and Variance:\n","\n","The mean exists only if d2 > 2, given by d2/(d2-2)\n","\n","The variance exists only if d2 > 4, and it decreases as d2\n","  increases.\n","\n","\n","\n","4. Relation to Other Distributions: The F-distribution is based on the ratio of two independent chi-square variables divided by their degrees of freedom. As the degrees of freedom grow large, the F-distribution approaches a normal distribution.\n","\n","5. Application in Hypothesis Testing: It is primarily used in ANOVA and regression to compare variances, helping test whether group means are significantly different.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"fQB879l-Orzr"}},{"cell_type":"markdown","source":["Q2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n","\n","\n"],"metadata":{"id":"eVntGL0zQDPJ"}},{"cell_type":"markdown","source":["The F-distribution is primarily used in statistical tests that involve comparing variances, specifically in:\n","\n","1. Analysis of Variance (ANOVA):\n","\n","Purpose: ANOVA tests are used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others.\n","\n","Why F-distribution: In ANOVA, the F-distribution helps compare the variance between group means (between-group variability) to the variance within each group (within-group variability). If the between-group variance is significantly higher than the within-group variance, it suggests that not all group means are equal.\n","2. Regression Analysis:\n","\n","Purpose: In regression, the F-test evaluates the overall significance of a regression model, checking whether the relationship between the dependent and independent variables is statistically significant.\n","\n","Why F-distribution: The F-distribution tests the ratio of variance explained by the model (regression sum of squares) to the unexplained variance (error sum of squares). A higher F-statistic indicates that the model significantly explains the variation in the dependent variable.\n","3. Comparing Two Variances:\n","\n","Purpose: F-tests can directly compare the variances of two independent populations to see if they are significantly different.\n","\n","Why F-distribution: The F-distribution is suitable because it is derived from the ratio of two independent sample variances. By comparing this ratio to the F-distribution, we can assess whether the observed variances differ significantly.\n","\n","The F-distribution is appropriate for these tests because it models the distribution of variance ratios. Since these tests involve comparing the variability between groups or explaining variability in a model, the F-distribution provides a framework for assessing whether observed differences in variability are statistically meaningful.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"LocKer4cQMfi"}},{"cell_type":"markdown","source":["Q3. What are the key assumptions required for conducting an F-test to compare the variances of two\n","populations?\n","\n","\n"],"metadata":{"id":"cV8qPJtpQhiw"}},{"cell_type":"markdown","source":["1. Normality: The data in each population should be normally distributed. The F-test is sensitive to deviations from normality, especially with smaller sample sizes. If this assumption is violated, the test results may not be accurate.\n","\n","2. Independence: The samples from each population must be independent of each other. This means that the observations in one sample do not influence the observations in the other.\n","\n","3. Random Sampling: The data should be collected through random sampling or be representative of the populations. This helps ensure that the samples accurately reflect the true population characteristics.\n","\n","4. Measurement Scale: The data should be measured on an interval or ratio scale (i.e., data with meaningful distances between values), which is essential for calculating variances."],"metadata":{"id":"XXPQQiHuQoNq"}},{"cell_type":"markdown","source":["Q4. What is the purpose of ANOVA, and how does it differ from a t-test?"],"metadata":{"id":"0czo-XdAQ6PA"}},{"cell_type":"markdown","source":["Purpose of ANOVA\n","ANOVA (Analysis of Variance) is a statistical technique used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others. The primary purposes of ANOVA include:\n","\n","1. Testing for Differences: ANOVA helps identify whether the differences in means observed among multiple groups are statistically significant.\n","\n","2. Controlling Type I Error: By testing all groups simultaneously, ANOVA controls the overall Type I error rate (the probability of incorrectly rejecting the null hypothesis) better than performing multiple t-tests.\n","\n","3. Understanding Variability: ANOVA separates the total variability observed in the data into components attributable to different sources (between-group variability and within-group variability), providing insight into the structure of the data.\n","\n"],"metadata":{"id":"olopeupeQ-xu"}},{"cell_type":"markdown","source":["1. Number of Groups:\n","\n","ANOVA: Used to compare the means of three or more groups.\n","\n","t-test: Used to compare the means of two groups only.\n","2. Type of Hypothesis:\n","\n","ANOVA: Tests the null hypothesis that all group means are equal (e.g.,\n","ð»\n","0\n",":\n","ðœ‡\n","1\n","=\n","ðœ‡\n","2\n","=\n","ðœ‡\n","3\n"," ).\n","\n","t-test: Tests the null hypothesis that the means of two groups are equal (e.g.,\n","ð»\n","0\n",":\n","ðœ‡\n","1\n","=\n","ðœ‡\n","2\n"," ).\n","3. Multiple Comparisons:\n","\n","ANOVA: If ANOVA indicates significant differences, post-hoc tests (e.g., Tukey's HSD) are often conducted to identify which specific groups differ.\n","\n","t-test: Directly provides the comparison between two groups without the need for additional tests.\n","\n","4. Statistical Model:\n","\n","ANOVA: Analyzes the variance among groups and is often used in experimental designs where factors are manipulated.\n","\n","t-test: Analyzes the means directly and is commonly used in simpler comparative studies.\n"],"metadata":{"id":"k6qhCXZaRWBu"}},{"cell_type":"markdown","source":["Q5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n","than two groups."],"metadata":{"id":"Y7lSMyxSSqo5"}},{"cell_type":"markdown","source":["Using a one-way ANOVA instead of multiple t-tests when comparing more than two groups is advisable for several reasons:\n","\n","When to Use One-Way ANOVA\n","More Than Two Groups: One-way ANOVA is specifically designed for situations where you need to compare the means of three or more groups. For example, if you have three different treatments or conditions and you want to see if they produce different outcomes, one-way ANOVA is appropriate.\n","Why Use One-Way ANOVA\n","1. Control Type I Error Rate: When conducting multiple t-tests, the probability of making at least one Type I error (incorrectly rejecting a true null hypothesis) increases with each additional test. One-way ANOVA allows for a single test to assess differences across all groups, thus controlling the overall Type I error rate. For example, if you conduct five t-tests, each with a 5% significance level, the overall error rate increases significantly.\n","\n","2. Efficiency: One-way ANOVA is more efficient than conducting multiple t-tests. It reduces the number of statistical tests performed, simplifying the analysis and interpretation of results.\n","\n","3. Overall Variability Assessment: One-way ANOVA assesses the overall variability among groups by partitioning the total variance into between-group variance and within-group variance. This provides a comprehensive understanding of how group means compare and the extent to which they differ.\n","\n","4. Post-Hoc Comparisons: If the one-way ANOVA indicates significant differences, you can conduct post-hoc tests (such as Tukey's HSD or Bonferroni correction) to determine specifically which group means differ. This structured approach is more robust than simply conducting multiple pairwise t-tests.\n","\n","5. Underlying Assumptions: One-way ANOVA assumes that the groups are independent and normally distributed, with equal variances (homoscedasticity). These assumptions can be more easily assessed in a single analysis than across multiple t-tests.\n","\n"],"metadata":{"id":"iY0tR9XqUHp1"}},{"cell_type":"markdown","source":["Q6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n","How does this partitioning contribute to the calculation of the F-statistic?"],"metadata":{"id":"3mTt2aPGUYTd"}},{"cell_type":"markdown","source":["Partitioning Variance in ANOVA\n","\n","In ANOVA (Analysis of Variance), the total variance observed in a dataset is divided into two main components: between-group variance and within-group variance. This division helps us understand how the variability in the data arises from differences between groups versus random variation within each group.\n","\n","1. Between-Group Variance:\n","\n","This component measures the variability due to the differences between the means of different groups. It reflects how much the group means deviate from the overall mean of all observations. If the group means are significantly different from each other, the between-group variance will be high.\n","2. Within-Group Variance:\n","\n","This component measures the variability within each group. It looks at how individual observations within a group differ from the group's mean. A small within-group variance indicates that the observations are closely clustered around the group mean, while a large within-group variance suggests more spread among the observations.\n","\n","The total variance in the data can be seen as the combination of these two components.\n","\n","\n","Contribution to the F-statistic\n","\n","The F-statistic in ANOVA is calculated as the ratio of the average between-group variance to the average within-group variance. This statistic is crucial for determining whether the differences among group means are statistically significant.\n","\n","A higher F-statistic indicates that the variability between group means is greater than the variability within groups. This suggests that at least one group mean is significantly different from the others.\n","\n","Conversely, a low F-statistic suggests that any observed differences between group means could be due to random variation within the groups, rather than actual differences.\n"],"metadata":{"id":"Hk7sGiseVUDH"}},{"cell_type":"markdown","source":["Q7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n","differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"],"metadata":{"id":"wsBCjZCXVjWh"}},{"cell_type":"markdown","source":["1. Handling Uncertainty\n","\n","Frequentist Approach:\n","\n","\n","Uncertainty is expressed through the concept of confidence intervals and p-values. The focus is on long-run frequencies, where results are interpreted in terms of whether they would occur in repeated sampling.\n","The frequentist approach often leads to binary decisions (reject or fail to reject the null hypothesis) based on a predetermined significance level (e.g., 0.05).\n","\n","Bayesian Approach:\n","\n","Uncertainty is quantified using probability distributions. Bayesian methods provide a direct probability statement about parameters and hypotheses (e.g., the probability that a parameter lies within a certain range).\n","Bayesian analysis results in posterior distributions that represent updated beliefs about parameters after observing the data, allowing for more nuanced interpretations of uncertainty.\n","2. Parameter Estimation\n","\n","Frequentist Approach:\n","\n","Parameters are estimated using point estimates (e.g., sample means) and typically involve methods like least squares. Confidence intervals provide a range of values for the parameter but do not convey the probability of the parameter being in that range.\n","Frequentist estimations do not incorporate prior information about the parameters; they rely solely on the data from the current experiment.\n","\n","Bayesian Approach:\n","\n","Parameters are treated as random variables with their own probability distributions. Bayesian estimation combines prior beliefs (prior distributions) with observed data to produce posterior distributions.\n","This allows for the incorporation of prior knowledge or expert opinions into the analysis, which can be particularly useful when data is scarce or uncertain.\n","\n","3. Hypothesis Testing\n","\n","Frequentist Approach:\n","\n","Hypothesis testing is based on p-values, which indicate the strength of evidence against the null hypothesis. A low p-value suggests strong evidence to reject the null hypothesis, while a high p-value indicates insufficient evidence.\n","The null hypothesis is often a specific point hypothesis (e.g., no difference in means), and the analysis does not provide direct probabilities for the hypotheses themselves.\n","\n","Bayesian Approach:\n","\n","Hypothesis testing is conducted by comparing the posterior probabilities of different hypotheses. This allows for direct probability statements regarding the hypotheses (e.g., the probability that the treatment effect is greater than zero).\n","\n","The Bayesian framework can accommodate complex hypotheses and models, allowing for a more flexible and interpretable approach to hypothesis testing."],"metadata":{"id":"wQ255htWVm07"}},{"cell_type":"markdown","source":["Q8. Question2 You have two sets of data representing the incomes of two different professions:\n","\n"," Profession A :  [48, 52, 55, 60, 62]\n","\n"," Profession B : [45, 50, 55, 52, 47]\n","Perform an F-test to determine if the variances of the two professions'\n","incomes are equal. What are your conclusions based on the F-test?\n","\n","Task: Use Python to calculate the F-statistic and p-value for the given data.\n","\n","Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."],"metadata":{"id":"jPHM9fxJWEG6"}},{"cell_type":"code","source":["import numpy as np\n","from scipy import stats\n","\n","profession_a = [48, 52, 55, 60, 62]\n","profession_b = [45, 50, 55, 52, 47]\n","\n","var_a = np.var(profession_a, ddof = 1)\n","var_b = np.var(profession_b, ddof = 1)\n","\n","f_statistics = var_a / var_b\n","\n","df_a = len(profession_a) - 1\n","df_b = len(profession_b) - 1\n","\n","aplha = 0.05\n","\n","critical_value = stats.f.ppf(q = 1 - aplha, dfn = df_a, dfd = df_b)\n","\n","if f_statistics > critical_value:\n","    print(\"Reject the null hypothesis, the variances are significantly different\")\n","\n","else:\n","    print(\"Fail to reject the null hypothesis, the variances are not significantly different\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HXYBX8qfOrKI","executionInfo":{"status":"ok","timestamp":1730727231458,"user_tz":-330,"elapsed":3335,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"922e8f26-1112-42a6-9463-611556e9e611"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Fail to reject the null hypothesis, the variances are not significantly different\n"]}]},{"cell_type":"code","source":["p_value = stats.f.sf(f_statistics, dfn = df_a, dfd = df_b)\n","p_value"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2qKHVZMeYP_P","executionInfo":{"status":"ok","timestamp":1730727504555,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"eb034686-b68a-4c5f-a1c1-0ee406e1b094"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.24652429950266966"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["Q9. Question2 Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n","average heights between three different regions with the following data\n","\n","Region A: [160, 162, 165, 158, 164]\n","\n","Region B: [172, 175, 170, 168, 174]\n","\n","Region C: [180, 182, 179, 185, 183]\n","\n","Task: Write Python code to perform the one-way ANOVA and interpret the results\n","\n","Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."],"metadata":{"id":"LLh1uovbZl0q"}},{"cell_type":"code","source":["region_a = [160, 162, 165, 158, 164]\n","region_b = [172, 175, 170, 168, 174]\n","region_c = [180, 182, 179, 185, 183]\n","\n","f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n","\n","print(f\"F-statistic: {f_statistic:.4f}\")\n","print(f\"P-value: {p_value:.4f}\")\n","\n","alpha = 0.05\n","if p_value < alpha:\n","    print(\"Reject the null hypothesis: There are statistically significant differences in average heights between the regions.\")\n","else:\n","    print(\"Fail to reject the null hypothesis: There is no statistically significant difference in average heights between the regions.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OHczLoobZeaA","executionInfo":{"status":"ok","timestamp":1730727819418,"user_tz":-330,"elapsed":425,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"7273fdac-42f5-488a-9b17-2be352532065"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["F-statistic: 67.8733\n","P-value: 0.0000\n","Reject the null hypothesis: There are statistically significant differences in average heights between the regions.\n"]}]}]}